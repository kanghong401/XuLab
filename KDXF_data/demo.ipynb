{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "统计数据 长度 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f333337158437a8d39accdffdecb1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/extra1T/model_embeding/ZhipuAI/glm-4-9b-chat\",trust_remote_code=True,)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/home/extra1T/model_embeding/ZhipuAI/glm-4-9b-chat\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True\n",
    ").to(device).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'matrix': (28.000008, 0.0, 0.0, 28.000008, 483.3716, 428.88), 'fontname': 'AAAAAH+DengXian-Bold', 'adv': 1.0, 'upright': True, 'x0': 483.3716, 'y0': 422.383998144, 'x1': 511.371608, 'y1': 450.384006144, 'width': 28.00000799999998, 'height': 28.000008000000037, 'size': 28.000008000000037, 'mcid': None, 'tag': None, 'object_type': 'char', 'page_number': 2, 'ncs': 'ICCBased', 'text': '录', 'stroking_color': None, 'stroking_pattern': None, 'non_stroking_color': (1, 1, 1), 'non_stroking_pattern': None, 'top': 89.61599385599999, 'bottom': 117.61600185600003, 'doctop': 629.6159938559999}\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "with pdfplumber.open(\"/home/extra1T/kangh/app/slides.pdf\") as pdf:\n",
    "    first_page = pdf.pages[1]\n",
    "    print(first_page.chars[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "def get_weather(city_id):\n",
    "    api_key = '451f1780d0d5ac1ac14ea48010d04f4d'\n",
    "    url = f'https://dev.qweather.com/v7/weather/now?city={city_id}&key={api_key}'\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    if data['status'] == 200:\n",
    "        weather_info = {\n",
    "            'temperature': data['now']['tmp'],\n",
    "            'humidity': data['now']['hum'],\n",
    "            'wind_speed': data['now']['wind_spd'],\n",
    "        }\n",
    "        return weather_info\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04 文 本 理 解\n",
      "pdfplumber抽取文字、表格 首页或页眉获取招股书公司名 去除页眉页脚\n",
      "数据预处理\n",
      "保存为以公司名为标题的txt文件 转换表格数据为制表符间隔的文本 还原段落\n",
      "公司名匹配 原公司名字符串匹配 bm25相似度召回，设置阈值 全文档字符串匹配\n",
      "文本分割 langchain的RecursiveCharacterTextSplitter\n",
      "提示词如下：\n",
      "参数：chunk_size: 200 chunk_overlap: 100\n",
      "------检索内容开始------\n",
      "{extra_knowledge}\n",
      "内容召回 bm25相似度召回，top_k：5,10,15,20,30\n",
      "------检索内容结束------\n",
      "用户问题：{user_question}。\n",
      "完全根据检索内容结合问题回答用户问题，将问题和答案结合后\n",
      "输出。注意不要输出“根据检索”。\n",
      "模型输出 将检索信息和用户问题传入大模型得到输出\n",
      "extra_knowledge: 检索到的信息 user_question: 用户问题\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "\n",
    "with pdfplumber.open(\"/home/extra1T/kangh/app/slides.pdf\") as pdf:\n",
    "    page = pdf.pages[10]   # 第一页的信息\n",
    "    text = page.extract_text()\n",
    "    print(text)\n",
    "    table = page.extract_tables()\n",
    "    for t in table:\n",
    "        # 得到的table是嵌套list类型，转化成DataFrame更加方便查看和分析\n",
    "        df = pd.DataFrame(t[1:], columns=t[0])\n",
    "        print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# 读取输入的 CSV 文件\n",
    "input_file = '/home/extra1T/jin/GoMate/test_question copy.csv'\n",
    "df = pd.read_csv(input_file)\n",
    "input_file1 = '/home/extra1T/jin/GoMate/test_question.csv'\n",
    "df1 = pd.read_csv(input_file1)\n",
    "# 创建输出 CSV 文件路径\n",
    "output_file = '/home/extra1T/jin/GoMate/output_answers2.csv'\n",
    "\n",
    "# 打开输出 CSV 文件，准备写入\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    # 写入表头\n",
    "    writer.writerow(['question'])\n",
    "    \n",
    "    for row in range(265,len(df)):\n",
    "        question = df['question'][row]\n",
    "        question1 = df1['question'][row]\n",
    "        messages = [{'role': 'user', 'content': question}]\n",
    "        answers = gpt_35_api(messages)  # 返回列表类型\n",
    "        answers = question1+answers\n",
    "        \n",
    "        combined_result = answers\n",
    "        \n",
    "        # 将组合后的字符串写入 CSV 文件\n",
    "        writer.writerow([combined_result])\n",
    "        # print(f\"Processed: {combined_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'你是一名高级智能助手，你需要根据当前提供的信息，执行当前任务。当前任务可以使用的插件信息如下，请自行判断是否需要调用插件来解决当前用户问题。若需要调用插件，则需要将插件调用请求按照json格式给出，必须包含api_name、parameters字段，并在其前后使用<|startofthink|>和<|endofthink|>作为标志。若无需调用插件，直接执行任务，结果无需标志。\\n1. {\"name\":\"DocumentRetrieve\",\"description\":\"招股书文档检索\",\"parameters\":[{\"name\":\"company\",\"description\":\"公司\",\"required\":false}]}\\n\\n2. {\"name\":\"SqlGenerator\",\"description\":\"sql语句生成\",\"parameters\":[{\"name\":\"table_list\",\"description\":\"所需表\",\"required\":true}]}\\n\\n3. {\"name\":\"DatabaseQuery\",\"description\":\"数据库查询\",\"parameters\":[{\"name\":\"sql_sentence\",\"description\":\"sql语句\",\"required\":true}]}\\n\\n用户问题：云南沃森生物技术股份有限公司负责产品研发的是什么部门？\\n\\n当前任务：查看云南沃森生物技术股份有限公司的公司招股书，寻找产品研发相关的信息。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一行的长度为: 181\n",
      "总共有 50000 行文本\n",
      "平均长度为: 900.67\n",
      "最短行的长度为: 8\n",
      "最长行的长度为: 46909，位于第 37969 行\n"
     ]
    }
   ],
   "source": [
    "def analyze_txt_file(file_path):\n",
    "    lengths = []\n",
    "    # 项目\n",
    "    # 读取文件内容并计算每行的长度\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line_length = len(line.strip())  # 去除首尾空格\n",
    "            if line_length > 0:  # 过滤掉空行\n",
    "                lengths.append(line_length)\n",
    "    \n",
    "    # 计算平均长度、最小长度和最大长度\n",
    "    if lengths:\n",
    "        avg_length = sum(lengths) / len(lengths)\n",
    "        min_length = min(lengths)\n",
    "        max_length = max(lengths)\n",
    "        max_line_number = lengths.index(max_length) + 1  # 行数从1开始\n",
    "        \n",
    "        print(f\"第一行的长度为: {lengths[0]}\")\n",
    "        print(f\"总共有 {len(lengths)} 行文本\")\n",
    "        print(f\"平均长度为: {avg_length:.2f}\")\n",
    "        print(f\"最短行的长度为: {min_length}\")\n",
    "        print(f\"最长行的长度为: {max_length}，位于第 {max_line_number} 行\")\n",
    "    else:\n",
    "        print(\"没有有效的文本行\")\n",
    "\n",
    "# 调用函数并传入你的文件路径\n",
    "file_path = '/home/extra1T/jin/app/corpus.txt'\n",
    "analyze_txt_file(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "SOURCE_INDEX_DATA_PATH = \"/home/extra1T/model_embeding/baike_qa_train.json\"\n",
    "OUTPUT_PATH = '/home/extra1T/kangh/app/kh/baike_qa.json'\n",
    "\n",
    "# 加载并处理数据\n",
    "source_index_data = []\n",
    "with open(SOURCE_INDEX_DATA_PATH, encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        ll = json.loads(line.strip())\n",
    "        if len(ll[\"title\"]) >= 2:\n",
    "            source_index_data.append([ll[\"title\"], ll])\n",
    "        if len(ll[\"desc\"]) >= 2:\n",
    "            source_index_data.append([ll[\"desc\"], ll])\n",
    "\n",
    "# 只取前200000条数据\n",
    "source_index_data = source_index_data[:100]\n",
    "\n",
    "# 保存为json文件\n",
    "with open(OUTPUT_PATH, 'w', encoding='utf8') as outfile:\n",
    "    json.dump(source_index_data, outfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"数据已保存到 {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载并且调用simcse-chinese-roberta-wwm-ext 向量模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/extra1T/kangh/miniconda3/agent/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101,  872, 1962, 1435,  102,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101,  872, 3221, 6443,  102,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.7139, -0.2214,  0.4365,  ..., -0.2937,  0.0213, -0.4205],\n",
      "         [-0.4433, -1.3391,  0.2240,  ...,  0.6484, -0.1123, -1.0580],\n",
      "         [-0.7925, -0.1824, -0.0316,  ...,  0.4552,  0.2713, -0.6319],\n",
      "         ...,\n",
      "         [ 0.0390, -0.1023,  0.0786,  ...,  0.2885, -0.2609, -0.4990],\n",
      "         [ 0.0827, -0.1568,  0.0979,  ...,  0.2859, -0.2736, -0.4798],\n",
      "         [ 0.0782, -0.1186,  0.0780,  ...,  0.2962, -0.2186, -0.4531]],\n",
      "\n",
      "        [[ 0.3579,  0.1189,  0.4538,  ..., -0.6430, -0.0103,  0.0410],\n",
      "         [ 0.3000, -1.0644,  0.7119,  ...,  0.8514,  0.5773, -0.8785],\n",
      "         [ 1.2434,  0.1731,  0.2852,  ...,  0.2180, -0.1818, -1.0287],\n",
      "         ...,\n",
      "         [ 0.4959,  0.0768,  0.1350,  ...,  0.0975, -0.3683, -0.2592],\n",
      "         [ 0.4449, -0.0725,  0.1269,  ...,  0.1557, -0.3151, -0.1337],\n",
      "         [ 0.4777, -0.0066,  0.0882,  ...,  0.2023, -0.3257, -0.2187]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.9830,  0.3013,  0.9329,  ..., -0.9302, -0.6719,  0.2546],\n",
      "        [ 0.9680, -0.4583,  0.9460,  ..., -0.7589, -0.5847,  0.4248]],\n",
      "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import BertConfig, BertModel, BertTokenizer\n",
    "# 使用本地模型路径\n",
    "model_path = \"/home/extra1T/model_embeding/simcse-chinese-roberta-wwm-ext\"\n",
    "query = ['你好呀','你是谁']\n",
    "model = BertModel.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "q_id = tokenizer(query, max_length = 200, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "out = model(**q_id)\n",
    "print(q_id)\n",
    "print(out)\n",
    "out.last_hidden_state[:, 0].shape\n",
    "# # 初始化 HuggingFaceEmbeddings，使用本地模型路径\n",
    "# embedding = HuggingFaceEmbeddings(model_name=model_path)\n",
    "\n",
    "# # 使用 Chroma 向量存储（示例，假设要创建 Chroma 数据库）\n",
    "# vector_store = Chroma(embedding_function=embedding)\n",
    "\n",
    "# # 示例：将文本转换为嵌入向量\n",
    "# texts = [\"今天天气很好\", \"我喜欢学习自然语言处理\"]\n",
    "# embeddings = [embedding.embed(text) for text in texts]\n",
    "\n",
    "# # 打印嵌入\n",
    "# print(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "视频里面的多路召回融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bge = json.load(open('submit bge sgement retrieval top10.json'))\n",
    "bm25 = json.load(open('submit bm25 retrieval top10.json'))\n",
    "fusion_result =[]\n",
    "k=60\n",
    "for q1,q2 in zip(bge, bm25):\n",
    "    fusion_score ={}\n",
    "    for idx,q in enumerate(q1['reference']):\n",
    "            if q not in fusion_score:\n",
    "                fusion_score[q]=1/(idx+ k)\n",
    "            else:\n",
    "                fusion_score[q]+=1/(idx + k)\n",
    "    for idx,q in enumerate(q2['reference']):\n",
    "            if q not in fusion_score:\n",
    "                fusion_score[q]=1/(idx+ k)\n",
    "            else:\n",
    "                fusion_score[q]+=1/(idx + k)     \n",
    "sorted_dict = sorted(fusion_score.items(), key=lambda item: item[1], reverse=True)\n",
    "q1['reference'] = sorted_dict[0][0]\n",
    "fusion_result.append(q1)\n",
    "\n",
    "#bge 和 bm25的形状\n",
    "# bge = [\n",
    "#     {\"reference\": [\"page_1\", \"page_2\", \"page_3\"]},\n",
    "#     {\"reference\": [\"page_5\", \"page_1\", \"page_4\"]}\n",
    "# ]\n",
    "\n",
    "# bm25 = [\n",
    "#     {\"reference\": [\"page_2\", \"page_1\", \"page_4\"]},\n",
    "#     {\"reference\": [\"page_1\", \"page_6\", \"page_5\"]}\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BM25实现 方法 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "import torch\n",
    "# 加载并处理文本\n",
    "def load_and_process_txt(file_path):\n",
    "    loader = TextLoader(file_path, encoding='utf8')\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # 分割文本，保持上下文一致性\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    return docs\n",
    "\n",
    "# 进行分词处理\n",
    "def process_for_bm25(docs):\n",
    "    # 对每个文档进行分词，得到一个二维数组\n",
    "    tokenized_docs = [jieba.lcut(doc.page_content) for doc in docs]\n",
    "    return tokenized_docs\n",
    "\n",
    "# 文档路径\n",
    "file_path = '/home/extra1T/jin/app/corpus copy.txt'\n",
    "\n",
    "# 加载并分割文本\n",
    "docs = load_and_process_txt(file_path)\n",
    "# print(\"dsad:\",len(docs))\n",
    "# print(\"dsads:\",docs[0].page_content)\n",
    "# 对文档进行分词，准备给 BM25 使用\n",
    "tokenized_docs = process_for_bm25(docs)\n",
    "\n",
    "# print(\"dsad:\",tokenized_docs[0])\n",
    "\n",
    "# 初始化 BM25 模型\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "\n",
    "# 输入查询，查询的内容需要分词\n",
    "query = \"第8届南方新丝路模特大赛的招募活动是在哪里举行的？\"\n",
    "query_tokens = jieba.lcut(query)\n",
    "\n",
    "# 使用 BM25 进行检索，得到每个文档的相关性分数\n",
    "doc_scores = bm25.get_scores(query_tokens)\n",
    "print(len(doc_scores))\n",
    "\n",
    "# 设置要返回的 topk 个文档\n",
    "topk = 3  # 例如返回前3个相似答案\n",
    "\n",
    "# 获取分数最高的 topk 个文档索引\n",
    "topk_indices = doc_scores.argsort()[-topk:][::-1]\n",
    "\n",
    "# 获取分数最高的 topk 个文档内容\n",
    "topk_matches = [docs[idx].page_content for idx in topk_indices]\n",
    "\n",
    "# 输出 topk 个最相关的文档内容\n",
    "print(\"最相关的文档内容：\")\n",
    "for i, match in enumerate(topk_matches, 1):\n",
    "    print(f\"Top {i}:\\n{match}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading [config.json]: 100%|██████████| 1.40k/1.40k [00:00<00:00, 3.19kB/s]\n",
      "Downloading [configuration.json]: 100%|██████████| 36.0/36.0 [00:00<00:00, 95.4B/s]\n",
      "Downloading [configuration_chatglm.py]: 100%|██████████| 2.21k/2.21k [00:00<00:00, 3.57kB/s]\n",
      "Downloading [generation_config.json]: 100%|██████████| 207/207 [00:00<00:00, 792B/s]\n",
      "Downloading [LICENSE]: 100%|██████████| 6.34k/6.34k [00:00<00:00, 15.2kB/s]\n",
      "Downloading [model-00001-of-00010.safetensors]:  42%|████▏     | 778M/1.81G [05:54<49:00, 384kB/s]  2024-09-20 16:17:33,450 - modelscope - WARNING - Downloading: /home/extra1T/model_embeding/._____temp/ZhipuAI/glm-4-9b-chat/model-00001-of-00010.safetensors failed, reason: ('Connection broken: IncompleteRead(25418969 bytes read, 142353191 more expected)', IncompleteRead(25418969 bytes read, 142353191 more expected)) will retry\n",
      "Downloading [model-00001-of-00010.safetensors]: 1.84GB [31:19, 1.05MB/s]                            \n",
      "Downloading [model-00002-of-00010.safetensors]: 100%|██████████| 1.69G/1.69G [29:53<00:00, 1.01MB/s]\n",
      "Downloading [model-00003-of-00010.safetensors]: 100%|██████████| 1.83G/1.83G [20:30<00:00, 1.60MB/s]\n",
      "Downloading [model-00004-of-00010.safetensors]: 100%|██████████| 1.80G/1.80G [18:02<00:00, 1.78MB/s]\n",
      "Downloading [model-00005-of-00010.safetensors]: 100%|██████████| 1.69G/1.69G [14:15<00:00, 2.12MB/s]\n",
      "Downloading [model-00006-of-00010.safetensors]: 100%|██████████| 1.83G/1.83G [00:01<00:00, 1.22GB/s]\n",
      "Downloading [model-00007-of-00010.safetensors]: 100%|██████████| 1.80G/1.80G [11:15<00:00, 2.85MB/s]\n",
      "Downloading [model-00008-of-00010.safetensors]: 100%|██████████| 1.69G/1.69G [08:14<00:00, 3.67MB/s]\n",
      "Downloading [model-00009-of-00010.safetensors]: 100%|██████████| 1.83G/1.83G [15:50<00:00, 2.07MB/s]\n",
      "Downloading [model-00010-of-00010.safetensors]: 100%|██████████| 1.54G/1.54G [10:08<00:00, 2.71MB/s]\n",
      "Downloading [model.safetensors.index.json]: 100%|██████████| 28.4k/28.4k [00:00<00:00, 85.1kB/s]\n",
      "Downloading [modeling_chatglm.py]: 100%|██████████| 46.2k/46.2k [00:00<00:00, 151kB/s]\n",
      "Downloading [README.md]: 100%|██████████| 8.71k/8.71k [00:00<00:00, 18.6kB/s]\n",
      "Downloading [README_en.md]: 100%|██████████| 9.29k/9.29k [00:00<00:00, 33.5kB/s]\n",
      "Downloading [tokenization_chatglm.py]: 100%|██████████| 20.5k/20.5k [00:00<00:00, 49.3kB/s]\n",
      "Downloading [tokenizer.model]: 100%|██████████| 2.50M/2.50M [00:00<00:00, 6.26MB/s]\n",
      "Downloading [tokenizer_config.json]: 100%|██████████| 6.01k/6.01k [00:00<00:00, 9.24kB/s]\n"
     ]
    }
   ],
   "source": [
    "from modelscope.hub.snapshot_download import snapshot_download\n",
    "model_dir = snapshot_download('ZhipuAI/glm-4-9b-chat', cache_dir='/home/extra1T/model_embeding/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "def max_score(s: str) -> int:\n",
    "    # 计算字符串中 b 的总数量\n",
    "    total_b = s.count('b')\n",
    "    \n",
    "    # 初始化变量\n",
    "    left_a_count = 0\n",
    "    right_b_count = total_b\n",
    "    max_score = 0\n",
    "    \n",
    "    # 遍历字符串，尝试在每个位置进行分割\n",
    "    for i in range(len(s) - 1):  # 最后一个字符不能作为分割点\n",
    "        if s[i] == 'a':\n",
    "            left_a_count += 1\n",
    "        else:\n",
    "            right_b_count -= 1\n",
    "        \n",
    "        # 计算当前得分\n",
    "        current_score = left_a_count + right_b_count\n",
    "        \n",
    "        # 更新最大得分\n",
    "        max_score = max(max_score, current_score)\n",
    "    \n",
    "    return max_score\n",
    "\n",
    "# 示例测试\n",
    "s = \"abbbab\"\n",
    "print(max_score(s))  # 输出应为 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=\"sk-X2FikC3IChAiXXgE3bAO6mjhqDiXa6w2zsse0PGJUVnWGGEE\",\n",
    "    base_url=\"https://api.chatanywhere.tech\"\n",
    "    # base_url=\"https://api.chatanywhere.org/v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "import torch\n",
    "\n",
    "# 载入并处理文本\n",
    "def load_and_process_txt(file_path):\n",
    "    loader = TextLoader(file_path, encoding='utf8')\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # 分割文本，保持上下文一致性\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    return docs\n",
    "# FAISS 向量召回\n",
    "def create_faiss_index(docs):\n",
    "    embeddings_model = HuggingFaceEmbeddings(model_name='/home/extra1T/model_embeding/ai-modelscope/gte-large-zh')\n",
    "    vector_store = FAISS.from_documents(docs, embeddings_model)\n",
    "    return vector_store\n",
    "# 文档路径\n",
    "file_path = '/home/extra1T/jin/app/corpus copy.txt'\n",
    "\n",
    "# 加载并分割文本\n",
    "docs = load_and_process_txt(file_path)\n",
    "\n",
    "# 初始化 FAISS 向量检索\n",
    "vector_store = create_faiss_index(docs)\n",
    "\n",
    "query = \"第8届南方新丝路模特大赛的招募活动是在哪里举行的？\"\n",
    "\n",
    "# 使用 FAISS 进行检索，并返回文档和对应的索引编号\n",
    "result_with_score = vector_store.similarity_search_with_score(query, k=topk)\n",
    "faiss_topk_matches = [(res.page_content, docs.index(res)) for res, score in result_with_score]  # 直接访问 page_content\n",
    "\n",
    "print(\"FAISS 检索结果：\")\n",
    "for i, (content, idx) in enumerate(faiss_topk_matches, 1):\n",
    "    print(f\"Top {i}:\\nContent: {content}\\nIndex: {idx}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分别使用Faiss 和 BM25 单路召回的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "import torch\n",
    "\n",
    "# 载入并处理文本\n",
    "def load_and_process_txt(file_path):\n",
    "    loader = TextLoader(file_path, encoding='utf8')\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # 分割文本，保持上下文一致性\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    return docs\n",
    "\n",
    "# 分词处理\n",
    "def process_for_bm25(docs):\n",
    "    tokenized_docs = [jieba.lcut(doc.page_content) for doc in docs]\n",
    "    return tokenized_docs\n",
    "\n",
    "# FAISS 向量召回\n",
    "def create_faiss_index(docs):\n",
    "    embeddings_model = HuggingFaceEmbeddings(model_name='/home/extra1T/model_embeding/ai-modelscope/gte-large-zh')\n",
    "    vector_store = FAISS.from_documents(docs, embeddings_model)\n",
    "    return vector_store\n",
    "\n",
    "# 文档路径\n",
    "file_path = '/home/extra1T/jin/app/corpus copy.txt'\n",
    "\n",
    "# 加载并分割文本\n",
    "docs = load_and_process_txt(file_path)\n",
    "\n",
    "# 处理文档并进行 BM25 分词\n",
    "tokenized_docs = process_for_bm25(docs)\n",
    "\n",
    "# 初始化 BM25 模型\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "\n",
    "# 初始化 FAISS 向量检索\n",
    "vector_store = create_faiss_index(docs)\n",
    "\n",
    "# 输入查询，查询内容需要分词\n",
    "query = \"第8届南方新丝路模特大赛的招募活动是在哪里举行的？\"\n",
    "query_tokens = jieba.lcut(query)\n",
    "\n",
    "# 使用 BM25 进行检索\n",
    "bm25_doc_scores = bm25.get_scores(query_tokens)\n",
    "\n",
    "# 设置返回的 topk 个文档\n",
    "topk = 3\n",
    "\n",
    "# 获取 BM25 topk 文档和其索引编号\n",
    "bm25_topk_indices = bm25_doc_scores.argsort()[-topk:][::-1]\n",
    "bm25_topk_matches = [(docs[idx].page_content, idx) for idx in bm25_topk_indices]  # 返回内容和索引\n",
    "\n",
    "# 使用 FAISS 进行检索，并返回文档和对应的索引编号\n",
    "result_with_score = vector_store.similarity_search_with_score(query, k=topk)\n",
    "faiss_topk_matches = [(res.page_content, docs.index(res)) for res, score in result_with_score]  # 直接访问 page_content\n",
    "\n",
    "# 打印结果\n",
    "print(\"BM25 检索结果：\")\n",
    "for i, (content, idx) in enumerate(bm25_topk_matches, 1):\n",
    "    print(f\"Top {i}:\\nContent: {content}\\nIndex: {idx}\\n\")\n",
    "\n",
    "print(\"FAISS 检索结果：\")\n",
    "for i, (content, idx) in enumerate(faiss_topk_matches, 1):\n",
    "    print(f\"Top {i}:\\nContent: {content}\\nIndex: {idx}\\n\")\n",
    "\n",
    "# 返回结果\n",
    "bm25_topk_matches, faiss_topk_matches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用RRF进行多路召回后的融合 将BM25和Faiss进行多路召回融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# 载入并处理文本\n",
    "def load_and_process_txt(file_path):\n",
    "    loader = TextLoader(file_path, encoding='utf8')\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # 分割文本，保持上下文一致性\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    return docs\n",
    "\n",
    "# 分词处理\n",
    "def process_for_bm25(docs):\n",
    "    tokenized_docs = [jieba.lcut(doc.page_content) for doc in docs]\n",
    "    return tokenized_docs\n",
    "\n",
    "# FAISS 向量召回\n",
    "def create_faiss_index(docs):\n",
    "    embeddings_model = HuggingFaceEmbeddings(model_name='/home/extra1T/model_embeding/ai-modelscope/gte-large-zh')\n",
    "    vector_store = FAISS.from_documents(docs, embeddings_model)\n",
    "    return vector_store\n",
    "\n",
    "# RRF 融合算法实现\n",
    "def rrf_fusion(bm25_results, faiss_results, k=60):\n",
    "    fusion_scores = {}\n",
    "\n",
    "    # BM25 结果，计算 RRF 分数\n",
    "    for rank, (doc, idx) in enumerate(bm25_results):\n",
    "        if idx not in fusion_scores:\n",
    "            fusion_scores[idx] = 1 / (k + rank)\n",
    "        else:\n",
    "            fusion_scores[idx] += 1 / (k + rank)\n",
    "\n",
    "    # FAISS 结果，计算 RRF 分数\n",
    "    for rank, (doc, idx) in enumerate(faiss_results):\n",
    "        if idx not in fusion_scores:\n",
    "            fusion_scores[idx] = 1 / (k + rank)\n",
    "        else:\n",
    "            fusion_scores[idx] += 1 / (k + rank)\n",
    "\n",
    "    # 根据融合分数对文档进行排序\n",
    "    sorted_fusion_results = sorted(fusion_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "    return sorted_fusion_results\n",
    "\n",
    "# 文档路径\n",
    "file_path = '/home/extra1T/jin/app/corpus copy.txt'\n",
    "\n",
    "# 加载并分割文本\n",
    "docs = load_and_process_txt(file_path)\n",
    "\n",
    "# 处理文档并进行 BM25 分词\n",
    "tokenized_docs = process_for_bm25(docs)\n",
    "\n",
    "# 初始化 BM25 模型\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "\n",
    "# 初始化 FAISS 向量检索\n",
    "vector_store = create_faiss_index(docs)\n",
    "\n",
    "# 输入查询，查询内容需要分词\n",
    "query = \"第8届南方新丝路模特大赛的招募活动是在哪里举行的？\"\n",
    "query_tokens = jieba.lcut(query)\n",
    "\n",
    "# 使用 BM25 进行检索\n",
    "bm25_doc_scores = bm25.get_scores(query_tokens)\n",
    "\n",
    "# 设置返回的 top K 个文档\n",
    "# topk = 60\n",
    "\n",
    "# 获取 BM25 top K 文档和其索引编号\n",
    "bm25_topk_indices = bm25_doc_scores.argsort()[::-1]\n",
    "bm25_topk_matches = [(docs[idx].page_content, idx) for idx in bm25_topk_indices]\n",
    "\n",
    "# 使用 FAISS 进行检索，并返回文档和对应的索引编号\n",
    "result_with_score = vector_store.similarity_search_with_score(query, k=len(docs))\n",
    "faiss_topk_matches = [(res.page_content, docs.index(res)) for res, score in result_with_score]  # 直接访问 page_content\n",
    "# 不在 BM25 检索时限制 top K，获取全部文档的评分和索引\n",
    "\n",
    "# bm25_topk_matches\n",
    "# faiss_topk_matches\n",
    "# 使用 RRF 进行分数融合\n",
    "rrf_results = rrf_fusion(bm25_topk_matches, faiss_topk_matches, k=60)\n",
    "topk = 3\n",
    "# 输出最终融合后的 top K 结果\n",
    "print(\"RRF 融合结果：\")\n",
    "for idx, (doc_idx, score) in enumerate(rrf_results[:topk], 1):\n",
    "    print(f\"Top {idx}:\\nContent: {docs[doc_idx].page_content}\\nRRF Score: {score}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多路召回后使用bge-reranker-base进行重排序操作 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 BGE 模型进行重排序\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "# 使用 RRF 进行分数融合\n",
    "# rrf_results = rrf_fusion(bm25_topk_matches, faiss_topk_matches, k=60)\n",
    "\n",
    "# 获取融合后 top 3 的文档\n",
    "top3_docs = [docs[idx].page_content for idx, _ in rrf_results[:3]]\n",
    "def bge_rerank(query, docs):\n",
    "    # 加载模型和分词器\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"/home/extra1T/model_embeding/Xorbits/bge-reranker-base\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"/home/extra1T/model_embeding/Xorbits/bge-reranker-base\")\n",
    "    \n",
    "    # 预处理\n",
    "    inputs = tokenizer([query] * len(docs), docs, padding=True, truncation=True, return_tensors=\"pt\",max_length=512)\n",
    "    \n",
    "    # 使用模型进行推理\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # 获取重排序得分（logits），并按得分从高到低排序\n",
    "    scores = outputs.logits.squeeze().tolist()\n",
    "    reranked_results = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return reranked_results\n",
    "\n",
    "# 使用 BGE 模型对 RRF 融合后的 top3 进行重排序\n",
    "reranked_results = bge_rerank(query, top3_docs)\n",
    "\n",
    "# 输出重排序后的 top 3 结果\n",
    "print(\"重排序后的 Top 3 结果：\")\n",
    "for doc, score in reranked_results:\n",
    "    print(f\"文档: {doc}, 得分: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_fused_docs = [doc[0] for doc in reranked_results[:3]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "视频中的重排序逻辑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m normalize\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrank_bm25\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BM25Okapi\n\u001b[0;32m----> 7\u001b[0m questions \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/extra1T/kangh/app/kh/test_question.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m pdf \u001b[38;5;241m=\u001b[39m pdfplumber\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m初赛训练数据集.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m pdf_content \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/home/extra1T/kangh/miniconda3/agent/lib/python3.10/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_int\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_constant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/extra1T/kangh/miniconda3/agent/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/home/extra1T/kangh/miniconda3/agent/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/home/extra1T/kangh/miniconda3/agent/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import jieba, json, pdfplumber\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "questions = json.load(open(\"questions.json\"))\n",
    "\n",
    "pdf = pdfplumber.open(\"初赛训练数据集.pdf\")\n",
    "pdf_content = []\n",
    "for page_idx in range(len(pdf.pages)):\n",
    "    pdf_content.append({\n",
    "        'page': 'page_' + str(page_idx + 1),\n",
    "        'content': pdf.pages[page_idx].extract_text()\n",
    "    })\n",
    "\n",
    "# 加载重排序模型\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('/home/extra1T/model_embeding/Xorbits/bge-reranker-base')\n",
    "rerank_model = AutoModelForSequenceClassification.from_pretrained('/home/extra1T/model_embeding/Xorbits/bge-reranker-base')\n",
    "rerank_model.cuda()\n",
    "\n",
    "pdf_content_words = [jieba.lcut(x['content']) for x in pdf_content]\n",
    "bm25 = BM25Okapi(pdf_content_words)\n",
    "\n",
    "for query_idx in range(len(questions)):\n",
    "\t\t# 首先进行BM25检索\n",
    "    doc_scores = bm25.get_scores(jieba.lcut(questions[query_idx][\"question\"]))\n",
    "    max_score_page_idxs = doc_scores.argsort()[-3:]\n",
    "\t\t\n",
    "\t\t# top3进行重排序\n",
    "    pairs = []\n",
    "    for idx in max_score_page_idxs:\n",
    "        pairs.append([questions[query_idx][\"question\"], pdf_content[idx]['content']])\n",
    "\n",
    "    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "    with torch.no_grad():\n",
    "        inputs = {key: inputs[key].cuda() for key in inputs.keys()}\n",
    "        scores = rerank_model(**inputs, return_dict=True).logits.view(-1, ).float()\n",
    "\n",
    "    max_score_page_idx = max_score_page_idxs[scores.cpu().numpy().argmax()]\n",
    "    questions[query_idx]['reference'] = 'page_' + str(max_score_page_idx + 1)\n",
    "\n",
    "with open('submit.json', 'w', encoding='utf8') as up:\n",
    "    json.dump(questions, up, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看 FAISS 向量库内部计算相似度的索引方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#这是 FAISS 中的一种索引类型，IndexFlatL2 表示使用 欧氏距离（L2 距离） 进行相似度计算。\n",
    "#在 FAISS 中，IndexFlatL2 会存储所有的向量，并使用欧氏距离来进行相似度计算。值越小证明相似度越高 \n",
    "vector_store.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从建立好的 Faiss向量库中 进行查询 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"在昆明中院联合晋宁县法院进行的公判中，为什么李佳宜被判无期徒刑？\"\n",
    "result = vector_store.similarity_search(query ,k = 10)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "官方的Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "template = \"\"\"你是一个用于问答任务的助手。\n",
    "使用下面检索到的上下文片段来完整的回答用户的问题。\n",
    "如果你不知道答案，只需说你不知道。\n",
    "问题：{question}\n",
    "上下文：{context}\n",
    "答案：\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "# model_path = \"/home/extra1T/model_embeding/qwen/Qwen2-7B\"\n",
    "model_path = \"/home/extra1T/model_embeding/ZhipuAI/glm-4-9b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,device_map=\"auto\",offload_folder=\"offload\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' # 设置 GPU 编号，如果单机单卡指定一个，单机多卡指定多个 GPU 编号\n",
    "MODEL_PATH = \"/home/extra1T/model_embeding/ZhipuAI/glm-4-9b\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "\n",
    "query = \"你好\"\n",
    "\n",
    "inputs = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": query}],\n",
    "                                       add_generation_prompt=True,\n",
    "                                       tokenize=True,\n",
    "                                       return_tensors=\"pt\",\n",
    "                                       return_dict=True\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs.to(device)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_kwargs = {\"max_length\": 250, \"do_sample\": True, \"top_k\": 1}\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, **gen_kwargs)\n",
    "    outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "# 假设你已经有了经过融合和重排序后的 top 3 结果\n",
    "# 这里使用 rrf_results 中的 top 3 作为上下文文档\n",
    "topk_fused_docs = [doc[0] for doc in reranked_results[:3]]  # 获取融合后 top 3 的文档内容\n",
    "\n",
    "# 将 top 3 的文档内容连接起来作为上下文\n",
    "context = \"\\n\\n\".join(topk_fused_docs)\n",
    "\n",
    "# 定义 RAG 的 Prompt 模板，包含问题和上下文\n",
    "prompt_template = \"\"\"你是一个用于问答任务的助手。\n",
    "使用下面检索到的上下文片段来完整的回答用户的问题。\n",
    "如果你不知道答案，只需说你不知道。\n",
    "{context}\n",
    "\n",
    "问题: {question}\n",
    "回答:\"\"\"\n",
    "\n",
    "# 定义 Prompt\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "\n",
    "# 构建链条\n",
    "rag_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "# 输入查询问题\n",
    "query = \"第8届南方新丝路模特大赛的海选时间是什么？\"\n",
    "\n",
    "# 调用 RAG 链，传递经过多路召回和重排序后的 top 3 文档作为上下文\n",
    "raw_answer = rag_chain.run({\"context\": context, \"question\": query})\n",
    "\n",
    "# 输出答案\n",
    "print(raw_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "批量循环处理所有问题 脚本 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "import csv\n",
    "# 载入并处理文本\n",
    "def load_and_process_txt(file_path):\n",
    "    loader = TextLoader(file_path, encoding='utf8')\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # 分割文本，保持上下文一致性\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    return docs\n",
    "\n",
    "# 分词处理\n",
    "def process_for_bm25(docs):\n",
    "    tokenized_docs = [jieba.lcut(doc.page_content) for doc in docs]\n",
    "    return tokenized_docs\n",
    "\n",
    "# FAISS 向量召回\n",
    "def create_faiss_index(docs):\n",
    "    embeddings_model = HuggingFaceEmbeddings(model_name='/home/extra1T/model_embeding/ai-modelscope/gte-large-zh')\n",
    "    vector_store = FAISS.from_documents(docs, embeddings_model)\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RRF 融合算法实现\n",
    "def rrf_fusion(bm25_results, faiss_results, k=60):\n",
    "    fusion_scores = {}\n",
    "    for rank, (doc, idx) in enumerate(bm25_results):\n",
    "        if idx not in fusion_scores:\n",
    "            fusion_scores[idx] = 1 / (k + rank)\n",
    "        else:\n",
    "            fusion_scores[idx] += 1 / (k + rank)\n",
    "    for rank, (doc, idx) in enumerate(faiss_results):\n",
    "        if idx not in fusion_scores:\n",
    "            fusion_scores[idx] = 1 / (k + rank)\n",
    "        else:\n",
    "            fusion_scores[idx] += 1 / (k + rank)\n",
    "    sorted_fusion_results = sorted(fusion_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "    return sorted_fusion_results\n",
    "\n",
    "# BGE 重排序模型\n",
    "def bge_rerank(query, docs):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"/home/extra1T/model_embeding/Xorbits/bge-reranker-base\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"/home/extra1T/model_embeding/Xorbits/bge-reranker-base\")\n",
    "    inputs = tokenizer([query] * len(docs), docs, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    scores = outputs.logits.squeeze().tolist()\n",
    "    reranked_results = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
    "    return reranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主逻辑：循环处理每个问题，生成上下文并批量处理\n",
    "def process_questions(questions, docs, bm25, vector_store, topk=3):\n",
    "    final_contexts = []  # 用于保存所有问题的上下文\n",
    "\n",
    "    for query in questions:\n",
    "        query_tokens = jieba.lcut(query)\n",
    "\n",
    "        # BM25 召回\n",
    "        bm25_doc_scores = bm25.get_scores(query_tokens)\n",
    "        bm25_topk_indices = bm25_doc_scores.argsort()[::-1]\n",
    "        bm25_topk_matches = [(docs[idx].page_content, idx) for idx in bm25_topk_indices]\n",
    "\n",
    "        # FAISS 召回\n",
    "        result_with_score = vector_store.similarity_search_with_score(query, k=len(docs))\n",
    "        faiss_topk_matches = [(res.page_content, docs.index(res)) for res, score in result_with_score]\n",
    "\n",
    "        # RRF 融合\n",
    "        rrf_results = rrf_fusion(bm25_topk_matches, faiss_topk_matches, k=60)\n",
    "\n",
    "        # 取融合后的 Top 3\n",
    "        top3_docs = [docs[idx].page_content for idx, _ in rrf_results[:topk]]\n",
    "\n",
    "        # 使用 BGE 模型进行重排序\n",
    "        reranked_results = bge_rerank(query, top3_docs)\n",
    "\n",
    "        # 获取重排序后的 Top 3 文档内容\n",
    "        topk_fused_docs = [doc[0] for doc in reranked_results[:topk]]\n",
    "\n",
    "        # 将 Top 3 的文档内容拼接成上下文\n",
    "        context = \"\\n\\n\".join(topk_fused_docs)\n",
    "\n",
    "        # 将每个问题和对应的上下文信息保存\n",
    "        final_contexts.append({\"query\": query, \"context\": context})\n",
    "\n",
    "    return final_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载文档和 BM25/FAISS 初始化\n",
    "file_path = '/home/extra1T/jin/app/corpus copy.txt'\n",
    "docs = load_and_process_txt(file_path)\n",
    "tokenized_docs = process_for_bm25(docs)\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "vector_store = create_faiss_index(docs)\n",
    "\n",
    "def load_questions(file_path):\n",
    "    questions = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)  # 假设 CSV 文件有一列叫 \"questions\"\n",
    "        for row in reader:\n",
    "            questions.append(row['question'].strip())  # 读取并清理每一行的问题\n",
    "    return questions\n",
    "\n",
    "# 假设 questions.csv 文件路径为 '/path/to/questions.csv'\n",
    "questions_file_path = '/home/extra1T/jin/app/test_question copy.csv'\n",
    "questions = load_questions(questions_file_path)\n",
    "\n",
    "# 处理所有问题，获取上下文\n",
    "final_contexts = process_questions(questions, docs, bm25, vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "model_path = \"/home/extra1T/model_embeding/qwen/Qwen2-7B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,device_map=\"auto\",offload_folder=\"offload\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# 假设你的 LLM 已经初始化好了\n",
    "# llm = LLM()  # 这里的 LLM 实际上是你的语言模型实例\n",
    "\n",
    "prompt_template = \"\"\"你是一个用于问答任务的助手。\n",
    "使用下面检索到的上下文片段来完整的回答用户的问题。\n",
    "如果你不知道答案，只需说你不知道。\n",
    "{context}\n",
    "\n",
    "问题: {question}\n",
    "回答:\"\"\"\n",
    "\n",
    "# 定义 Prompt\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "# 构建链条\n",
    "rag_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "# 准备输出文件\n",
    "output_file_path = '/home/extra1T/jin/appanswers.csv'\n",
    "header = ['Question', 'Answer']\n",
    "\n",
    "# 存储问题和答案到 CSV 文件\n",
    "with open(output_file_path, 'w', encoding='utf-8', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)  # 写入表头\n",
    "\n",
    "    for entry in final_contexts:\n",
    "        query = entry['query']\n",
    "        context = entry['context']\n",
    "        \n",
    "        # 将问题和上下文传递给大模型\n",
    "        raw_answer = rag_chain.run({\"context\": context, \"question\": query})\n",
    "\n",
    "        # 将问题和答案写入到 CSV 文件\n",
    "        writer.writerow([query, raw_answer])\n",
    "\n",
    "print(f\"所有问题和答案已经写入到 {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义结构链条 结合LLM和prompt 以及结合到的知识库查询进行最终回答 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# 假设 retriever, prompt 和 llm 已经定义好了\n",
    "# 这里保留 rag_chain 逻辑\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 读取CSV文件\n",
    "input_csv_path = '/home/extra1T/jin/app/test_question copy.csv'  # 输入问题的CSV文件路径\n",
    "output_csv_path = '/home/extra1T/jin/app/output_answers.csv'  # 输出答案的CSV文件路径\n",
    "\n",
    "# 读取CSV中的问题列\n",
    "df = pd.read_csv(input_csv_path)\n",
    "\n",
    "# 假设CSV中问题的列名是 'question'\n",
    "questions = df['question'].tolist()\n",
    "\n",
    "# 创建一个空列表存储问题的答案\n",
    "answers = []\n",
    "\n",
    "# 循环遍历每个问题并调用 rag_chain.invoke(query)\n",
    "for question in questions:\n",
    "    print(f\"正在处理问题: {question}\")\n",
    "    try:\n",
    "        answer = rag_chain.invoke(question)\n",
    "        answers.append(answer)\n",
    "    except Exception as e:\n",
    "        print(f\"处理问题 '{question}' 时出错: {e}\")\n",
    "        answers.append(\"处理失败\")\n",
    "\n",
    "# 将问题和答案存储到新的DataFrame\n",
    "output_df = pd.DataFrame({\n",
    "    'answer': answers\n",
    "})\n",
    "\n",
    "# 将DataFrame写入新的CSV文件\n",
    "output_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"所有问题处理完毕，结果已保存到 {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMKV详细解释 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM\n",
    "########################################################################################################\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, suppress=True, linewidth=200)\n",
    "import types, torch\n",
    "from torch.nn import functional as F\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"/home/extra1T/model_embeding/20B_tokenizer.json\")\n",
    "\n",
    "args = types.SimpleNamespace()\n",
    "args.MODEL_NAME = '/home/extra1T/model_embeding/RWKV-4-Pile-430M-20220808-8066'\n",
    "args.n_layer = 24\n",
    "args.n_embd = 1024\n",
    "\n",
    "# context = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n",
    "context = \"On a sunset evening, a boy was waiting downstairs with a bouquet of flowers for the girl\"\n",
    "NUM_TRIALS = 3\n",
    "LENGTH_PER_TRIAL = 100\n",
    "TEMPERATURE = 1.0\n",
    "TOP_P = 0.85\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "class RWKV_RNN(torch.jit.ScriptModule):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.eval() # set torch to inference mode\n",
    "        \n",
    "        w = torch.load(args.MODEL_NAME + '.pth', map_location='cpu')\n",
    "        for k in w.keys():\n",
    "            if      '.time_' in k: w[k] = w[k].squeeze()\n",
    "            if '.time_decay' in k: w[k] = -torch.exp(w[k].float()) # the real time decay is like e^{-e^x}\n",
    "            else: w[k] = w[k].float() # convert to f32 type\n",
    "        \n",
    "        self.w = types.SimpleNamespace() # set self.w from w\n",
    "        self.w.blocks = {}\n",
    "        for k in w.keys(): # example: \"blocks.0.att.time_first\" => self.w.blocks[0].att.time_first\n",
    "            parts = k.split('.')\n",
    "            last = parts.pop()\n",
    "            here = self.w\n",
    "            for p in parts:\n",
    "                if p.isdigit():\n",
    "                    p = int(p)\n",
    "                    if p not in here: here[p] = types.SimpleNamespace()\n",
    "                    here = here[p]\n",
    "                else:\n",
    "                    if not hasattr(here, p): setattr(here, p, types.SimpleNamespace())\n",
    "                    here = getattr(here, p)\n",
    "            setattr(here, last, w[k])\n",
    "\n",
    "    def layer_norm(self, x, w):\n",
    "        return F.layer_norm(x, (self.args.n_embd,), weight=w.weight, bias=w.bias)\n",
    "\n",
    "    @torch.jit.script_method\n",
    "    def channel_mixing(self, x, state, i:int, time_mix_k, time_mix_r, kw, vw, rw):\n",
    "        xk = x * time_mix_k + state[5*i+0] * (1 - time_mix_k)\n",
    "        xr = x * time_mix_r + state[5*i+0] * (1 - time_mix_r)\n",
    "        state[5*i+0] = x\n",
    "        r = torch.sigmoid(rw @ xr)\n",
    "        k = torch.square(torch.relu(kw @ xk)) # square relu, primer paper\n",
    "        return r * (vw @ k)\n",
    "\n",
    "    @torch.jit.script_method\n",
    "    def time_mixing(self, x, state, i:int, time_mix_k, time_mix_v, time_mix_r, time_first, time_decay, kw, vw, rw, ow):\n",
    "        xk = x * time_mix_k + state[5*i+1] * (1 - time_mix_k)\n",
    "        xv = x * time_mix_v + state[5*i+1] * (1 - time_mix_v)\n",
    "        xr = x * time_mix_r + state[5*i+1] * (1 - time_mix_r)\n",
    "        state[5*i+1] = x\n",
    "        r = torch.sigmoid(rw @ xr)\n",
    "        k = kw @ xk\n",
    "        v = vw @ xv\n",
    "        \n",
    "        aa = state[5*i+2]\n",
    "        bb = state[5*i+3]\n",
    "        pp = state[5*i+4]\n",
    "        ww = time_first + k\n",
    "        qq = torch.maximum(pp, ww)\n",
    "        e1 = torch.exp(pp - qq)\n",
    "        e2 = torch.exp(ww - qq)\n",
    "        a = e1 * aa + e2 * v\n",
    "        b = e1 * bb + e2\n",
    "        wkv = a / b\n",
    "        ww = pp + time_decay\n",
    "        qq = torch.maximum(ww, k)\n",
    "        e1 = torch.exp(ww - qq)\n",
    "        e2 = torch.exp(k - qq)\n",
    "        state[5*i+2] = e1 * aa + e2 * v\n",
    "        state[5*i+3] = e1 * bb + e2\n",
    "        state[5*i+4] = qq\n",
    "        return ow @ (r * wkv)\n",
    "\n",
    "    def forward(self, token, state):\n",
    "        with torch.no_grad():\n",
    "            if state == None:\n",
    "                state = torch.zeros(self.args.n_layer * 5, self.args.n_embd)\n",
    "                for i in range(self.args.n_layer): state[5*i+4] = -1e30 # -infinity\n",
    "            \n",
    "            x = self.w.emb.weight[token]\n",
    "            x = self.layer_norm(x, self.w.blocks[0].ln0)\n",
    "            for i in range(self.args.n_layer):\n",
    "                att = self.w.blocks[i].att\n",
    "                x = x + self.time_mixing(self.layer_norm(x, self.w.blocks[i].ln1), state, i, \n",
    "                    att.time_mix_k, att.time_mix_v, att.time_mix_r, att.time_first, att.time_decay, \n",
    "                    att.key.weight, att.value.weight, att.receptance.weight, att.output.weight)\n",
    "                ffn = self.w.blocks[i].ffn\n",
    "                x = x + self.channel_mixing(self.layer_norm(x, self.w.blocks[i].ln2), state, i, \n",
    "                    ffn.time_mix_k, ffn.time_mix_r, \n",
    "                    ffn.key.weight, ffn.value.weight, ffn.receptance.weight)\n",
    "            \n",
    "            x = self.w.head.weight @ self.layer_norm(x, self.w.ln_out)\n",
    "            return x.float(), state\n",
    "\n",
    "##########################################################################################################\n",
    "\n",
    "def sample_logits(out, temperature=1.0, top_p=0.8):\n",
    "    probs = F.softmax(out, dim=-1).numpy()\n",
    "    sorted_probs = np.sort(probs)[::-1]\n",
    "    cumulative_probs = np.cumsum(sorted_probs)\n",
    "    cutoff = float(sorted_probs[np.argmax(cumulative_probs > top_p)])\n",
    "    probs[probs < cutoff] = 0\n",
    "    if temperature != 1.0:\n",
    "        probs = probs.pow(1.0 / temperature)\n",
    "    probs = probs / np.sum(probs)\n",
    "    out = np.random.choice(a=len(probs), p=probs)\n",
    "    return out\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "print(f'\\nUsing CPU. Loading {args.MODEL_NAME} ...')\n",
    "model = RWKV_RNN(args)\n",
    "\n",
    "print(f'\\nPreprocessing context (slow version. see v2/rwkv/model.py for fast version)')\n",
    "init_state = None\n",
    "for token in tokenizer.encode(context).ids:\n",
    "    init_out, init_state = model.forward(token, init_state)\n",
    "\n",
    "for TRIAL in range(NUM_TRIALS):\n",
    "    print(f'\\n\\n--[ Trial {TRIAL} ]-----------------', context, end=\"\")\n",
    "    all_tokens = []\n",
    "    out_last = 0\n",
    "    out, state = init_out.clone(), init_state.clone()\n",
    "    for i in range(LENGTH_PER_TRIAL):\n",
    "        token = sample_logits(out, TEMPERATURE, TOP_P)\n",
    "        all_tokens += [token]\n",
    "        tmp = tokenizer.decode(all_tokens[out_last:])\n",
    "        if '\\ufffd' not in tmp: # only print when we have a valid utf-8 string\n",
    "            print(tmp, end=\"\", flush=True)\n",
    "            out_last = i + 1\n",
    "        out, state = model.forward(token, state)       \n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_sum_of_second_elements(arr):\n",
    "    n = len(arr)\n",
    "    if n < 3:\n",
    "        return 0\n",
    "    dp = [0] * n\n",
    "    dp[2] = arr[1]  \n",
    "    for i in range(3, n):\n",
    "        dp[i] = dp[i-1]\n",
    "        dp[i] = max(dp[i], dp[i-3] + arr[i-1])\n",
    "\n",
    "    return dp[-1]\n",
    "\n",
    "n = int(input(\"请输入数组的长度: \"))\n",
    "arr = list(map(int, input(\"请输入数组元素: \").split()))\n",
    "\n",
    "print(max_sum_of_second_elements(arr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
